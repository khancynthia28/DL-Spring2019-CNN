{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "All_Conv.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khancynthia28/DL-Spring2019-CNN/blob/master/All_Conv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9xtsjmytHJG",
        "colab_type": "text"
      },
      "source": [
        "###Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHAiPa9DtRmZ",
        "colab_type": "code",
        "outputId": "deef2077-e155-42e9-e6ff-c9a14aec64a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Activation, Convolution2D, GlobalAveragePooling2D, merge\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import SGD\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Lambda\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import pandas\n",
        "import cv2\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPZkT_PXuGbr",
        "colab_type": "text"
      },
      "source": [
        "###Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjqF26Y6ux7-",
        "colab_type": "code",
        "outputId": "fd288a79-0b0a-4b2b-f2dc-01608d2e2e15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "K.set_image_dim_ordering('tf')\n",
        "\n",
        "batch_size = 32\n",
        "nb_classes = 10\n",
        "nb_epoch = 350\n",
        "\n",
        "\n",
        "rows, cols = 32, 32\n",
        "\n",
        "channels = 3\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "print('X_train shape:', X_train.shape)\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "print (X_train.shape[1:])\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 15s 0us/step\n",
            "X_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "(32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5bPnfMKvNnd",
        "colab_type": "text"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfqqzI1fvUPH",
        "colab_type": "code",
        "outputId": "0a063463-d063-4151-8aaf-2d29d3951f03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1176
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(96, 3, 3, border_mode = 'same', input_shape=(32, 32, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(96, 3, 3,border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(96, 3, 3, border_mode='same', subsample = (2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Convolution2D(192, 3, 3, border_mode = 'same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(192, 3, 3,border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(192, 3, 3,border_mode='same', subsample = (2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Convolution2D(192, 3, 3, border_mode = 'same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(192, 1, 1,border_mode='valid'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(10, 1, 1, border_mode='valid'))\n",
        "\n",
        "\n",
        "\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "print (model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3), input_shape=(32, 32, 3..., padding=\"same\")`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3), padding=\"same\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3), strides=(2, 2), padding=\"same\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3), padding=\"same\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3), padding=\"same\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3), strides=(2, 2), padding=\"same\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3), padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (1, 1), padding=\"valid\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), padding=\"valid\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 96)        2688      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 96)        83040     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 32, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 96)        83040     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 192)       166080    \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16, 16, 192)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 16, 16, 192)       331968    \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16, 16, 192)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 8, 8, 192)         331968    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 8, 8, 192)         331968    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 8, 8, 192)         37056     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 8, 8, 10)          1930      \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,369,738\n",
            "Trainable params: 1,369,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrNL9rhswBmJ",
        "colab_type": "text"
      },
      "source": [
        "### Fit Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfF4pkl2wH2o",
        "colab_type": "code",
        "outputId": "22cabeba-a35e-447e-fc44-ad0496cbf364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 23922
        }
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False) \n",
        "\n",
        "datagen.fit(X_train)\n",
        "filepath=\"weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max')\n",
        "\n",
        "callbacks_list = [checkpoint]\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "history_callback = model.fit_generator(datagen.flow(X_train, Y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                        samples_per_epoch=X_train.shape[0],\n",
        "                        nb_epoch=nb_epoch, validation_data=(X_test, Y_test), callbacks=callbacks_list)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., steps_per_epoch=1562, epochs=350)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/350\n",
            "1562/1562 [==============================] - 35s 22ms/step - loss: 1.9676 - acc: 0.2507 - val_loss: 1.6878 - val_acc: 0.3303\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.33030, saving model to weights.hdf5\n",
            "Epoch 2/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 1.5349 - acc: 0.4302 - val_loss: 1.3102 - val_acc: 0.5207\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.33030 to 0.52070, saving model to weights.hdf5\n",
            "Epoch 3/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 1.2771 - acc: 0.5373 - val_loss: 1.1928 - val_acc: 0.5743\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.52070 to 0.57430, saving model to weights.hdf5\n",
            "Epoch 4/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 1.1045 - acc: 0.6055 - val_loss: 1.0120 - val_acc: 0.6349\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.57430 to 0.63490, saving model to weights.hdf5\n",
            "Epoch 5/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.9810 - acc: 0.6507 - val_loss: 0.8955 - val_acc: 0.6840\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.63490 to 0.68400, saving model to weights.hdf5\n",
            "Epoch 6/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.8844 - acc: 0.6893 - val_loss: 0.8602 - val_acc: 0.7136\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.68400 to 0.71360, saving model to weights.hdf5\n",
            "Epoch 7/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.8151 - acc: 0.7143 - val_loss: 0.7563 - val_acc: 0.7469\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.71360 to 0.74690, saving model to weights.hdf5\n",
            "Epoch 8/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.7529 - acc: 0.7380 - val_loss: 0.6972 - val_acc: 0.7700\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.74690 to 0.77000, saving model to weights.hdf5\n",
            "Epoch 9/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.7035 - acc: 0.7558 - val_loss: 0.6551 - val_acc: 0.7769\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.77000 to 0.77690, saving model to weights.hdf5\n",
            "Epoch 10/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.6702 - acc: 0.7673 - val_loss: 0.6141 - val_acc: 0.7913\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.77690 to 0.79130, saving model to weights.hdf5\n",
            "Epoch 11/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.6434 - acc: 0.7743 - val_loss: 0.6074 - val_acc: 0.7975\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.79130 to 0.79750, saving model to weights.hdf5\n",
            "Epoch 12/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.6169 - acc: 0.7861 - val_loss: 0.6283 - val_acc: 0.7946\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.79750\n",
            "Epoch 13/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.5922 - acc: 0.7967 - val_loss: 0.5631 - val_acc: 0.8119\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.79750 to 0.81190, saving model to weights.hdf5\n",
            "Epoch 14/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.5635 - acc: 0.8054 - val_loss: 0.6108 - val_acc: 0.7985\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.81190\n",
            "Epoch 15/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.5498 - acc: 0.8097 - val_loss: 0.5579 - val_acc: 0.8185\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.81190 to 0.81850, saving model to weights.hdf5\n",
            "Epoch 16/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.5327 - acc: 0.8157 - val_loss: 0.5837 - val_acc: 0.8139\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.81850\n",
            "Epoch 17/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.5191 - acc: 0.8198 - val_loss: 0.5238 - val_acc: 0.8299\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.81850 to 0.82990, saving model to weights.hdf5\n",
            "Epoch 18/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.5071 - acc: 0.8241 - val_loss: 0.5606 - val_acc: 0.8257\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.82990\n",
            "Epoch 19/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.4955 - acc: 0.8284 - val_loss: 0.5500 - val_acc: 0.8291\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.82990\n",
            "Epoch 20/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.4831 - acc: 0.8341 - val_loss: 0.5775 - val_acc: 0.8229\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.82990\n",
            "Epoch 21/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.4723 - acc: 0.8357 - val_loss: 0.4975 - val_acc: 0.8346\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.82990 to 0.83460, saving model to weights.hdf5\n",
            "Epoch 22/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.4651 - acc: 0.8391 - val_loss: 0.5019 - val_acc: 0.8422\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.83460 to 0.84220, saving model to weights.hdf5\n",
            "Epoch 23/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.4538 - acc: 0.8418 - val_loss: 0.4899 - val_acc: 0.8453\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.84220 to 0.84530, saving model to weights.hdf5\n",
            "Epoch 24/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.4461 - acc: 0.8451 - val_loss: 0.4891 - val_acc: 0.8558\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.84530 to 0.85580, saving model to weights.hdf5\n",
            "Epoch 25/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.4365 - acc: 0.8488 - val_loss: 0.4752 - val_acc: 0.8528\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.85580\n",
            "Epoch 26/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.4280 - acc: 0.8519 - val_loss: 0.5354 - val_acc: 0.8373\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.85580\n",
            "Epoch 27/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.4218 - acc: 0.8548 - val_loss: 0.5018 - val_acc: 0.8468\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.85580\n",
            "Epoch 28/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.4219 - acc: 0.8523 - val_loss: 0.6003 - val_acc: 0.8317\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.85580\n",
            "Epoch 29/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.4106 - acc: 0.8556 - val_loss: 0.4611 - val_acc: 0.8570\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.85580 to 0.85700, saving model to weights.hdf5\n",
            "Epoch 30/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.4109 - acc: 0.8567 - val_loss: 0.4740 - val_acc: 0.8555\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.85700\n",
            "Epoch 31/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.4043 - acc: 0.8595 - val_loss: 0.4923 - val_acc: 0.8511\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.85700\n",
            "Epoch 32/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.4022 - acc: 0.8612 - val_loss: 0.4434 - val_acc: 0.8631\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.85700 to 0.86310, saving model to weights.hdf5\n",
            "Epoch 33/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.3934 - acc: 0.8617 - val_loss: 0.5310 - val_acc: 0.8428\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.86310\n",
            "Epoch 34/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.3909 - acc: 0.8643 - val_loss: 0.4601 - val_acc: 0.8599\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.86310\n",
            "Epoch 35/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.3902 - acc: 0.8642 - val_loss: 0.5330 - val_acc: 0.8410\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.86310\n",
            "Epoch 36/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.3834 - acc: 0.8671 - val_loss: 0.4809 - val_acc: 0.8585\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.86310\n",
            "Epoch 37/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.3725 - acc: 0.8710 - val_loss: 0.4772 - val_acc: 0.8590\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.86310\n",
            "Epoch 38/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.3744 - acc: 0.8694 - val_loss: 0.4170 - val_acc: 0.8655\n",
            "\n",
            "Epoch 00038: val_acc improved from 0.86310 to 0.86550, saving model to weights.hdf5\n",
            "Epoch 39/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.3739 - acc: 0.8694 - val_loss: 0.4526 - val_acc: 0.8721\n",
            "\n",
            "Epoch 00039: val_acc improved from 0.86550 to 0.87210, saving model to weights.hdf5\n",
            "Epoch 40/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.3684 - acc: 0.8721 - val_loss: 0.4648 - val_acc: 0.8671\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.87210\n",
            "Epoch 41/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.3591 - acc: 0.8762 - val_loss: 0.4778 - val_acc: 0.8572\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.87210\n",
            "Epoch 42/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.3617 - acc: 0.8740 - val_loss: 0.4481 - val_acc: 0.8647\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.87210\n",
            "Epoch 43/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.3561 - acc: 0.8775 - val_loss: 0.4568 - val_acc: 0.8670\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.87210\n",
            "Epoch 44/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.3580 - acc: 0.8757 - val_loss: 0.4626 - val_acc: 0.8665\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.87210\n",
            "Epoch 45/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.3590 - acc: 0.8756 - val_loss: 0.4787 - val_acc: 0.8611\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.87210\n",
            "Epoch 46/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.3559 - acc: 0.8765 - val_loss: 0.4879 - val_acc: 0.8626\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.87210\n",
            "Epoch 47/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.3471 - acc: 0.8782 - val_loss: 0.4963 - val_acc: 0.8549\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.87210\n",
            "Epoch 48/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.3504 - acc: 0.8776 - val_loss: 0.5077 - val_acc: 0.8598\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.87210\n",
            "Epoch 49/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.3431 - acc: 0.8807 - val_loss: 0.4270 - val_acc: 0.8715\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.87210\n",
            "Epoch 50/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.3415 - acc: 0.8790 - val_loss: 0.4375 - val_acc: 0.8670\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.87210\n",
            "Epoch 51/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.3393 - acc: 0.8817 - val_loss: 0.4396 - val_acc: 0.8673\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.87210\n",
            "Epoch 52/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.3359 - acc: 0.8836 - val_loss: 0.4862 - val_acc: 0.8661\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.87210\n",
            "Epoch 53/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.3395 - acc: 0.8832 - val_loss: 0.4467 - val_acc: 0.8698\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.87210\n",
            "Epoch 54/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.3264 - acc: 0.8857 - val_loss: 0.4435 - val_acc: 0.8771\n",
            "\n",
            "Epoch 00054: val_acc improved from 0.87210 to 0.87710, saving model to weights.hdf5\n",
            "Epoch 55/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.3297 - acc: 0.8853 - val_loss: 0.4402 - val_acc: 0.8637\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.87710\n",
            "Epoch 56/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.3241 - acc: 0.8883 - val_loss: 0.4311 - val_acc: 0.8728\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.87710\n",
            "Epoch 57/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.3285 - acc: 0.8860 - val_loss: 0.4248 - val_acc: 0.8747\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.87710\n",
            "Epoch 58/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.3232 - acc: 0.8880 - val_loss: 0.4549 - val_acc: 0.8736\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.87710\n",
            "Epoch 59/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.3216 - acc: 0.8887 - val_loss: 0.4622 - val_acc: 0.8692\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.87710\n",
            "Epoch 60/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.3278 - acc: 0.8867 - val_loss: 0.4421 - val_acc: 0.8788\n",
            "\n",
            "Epoch 00060: val_acc improved from 0.87710 to 0.87880, saving model to weights.hdf5\n",
            "Epoch 61/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.3242 - acc: 0.8869 - val_loss: 0.4634 - val_acc: 0.8687\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.87880\n",
            "Epoch 62/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.3172 - acc: 0.8897 - val_loss: 0.4660 - val_acc: 0.8745\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.87880\n",
            "Epoch 63/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.3173 - acc: 0.8899 - val_loss: 0.4181 - val_acc: 0.8745\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.87880\n",
            "Epoch 64/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.3125 - acc: 0.8915 - val_loss: 0.4087 - val_acc: 0.8812\n",
            "\n",
            "Epoch 00064: val_acc improved from 0.87880 to 0.88120, saving model to weights.hdf5\n",
            "Epoch 65/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.3089 - acc: 0.8920 - val_loss: 0.4883 - val_acc: 0.8676\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.88120\n",
            "Epoch 66/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.3138 - acc: 0.8900 - val_loss: 0.4554 - val_acc: 0.8699\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.88120\n",
            "Epoch 67/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.3098 - acc: 0.8904 - val_loss: 0.4454 - val_acc: 0.8798\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.88120\n",
            "Epoch 68/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.3123 - acc: 0.8910 - val_loss: 0.4705 - val_acc: 0.8754\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.88120\n",
            "Epoch 69/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.3077 - acc: 0.8927 - val_loss: 0.4778 - val_acc: 0.8748\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.88120\n",
            "Epoch 70/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.3062 - acc: 0.8942 - val_loss: 0.4716 - val_acc: 0.8749\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.88120\n",
            "Epoch 71/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.3080 - acc: 0.8932 - val_loss: 0.4833 - val_acc: 0.8711\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.88120\n",
            "Epoch 72/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.3006 - acc: 0.8956 - val_loss: 0.4295 - val_acc: 0.8691\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.88120\n",
            "Epoch 73/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.3079 - acc: 0.8944 - val_loss: 0.4643 - val_acc: 0.8757\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.88120\n",
            "Epoch 74/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.2983 - acc: 0.8959 - val_loss: 0.4814 - val_acc: 0.8762\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.88120\n",
            "Epoch 75/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2978 - acc: 0.8964 - val_loss: 0.4269 - val_acc: 0.8836\n",
            "\n",
            "Epoch 00075: val_acc improved from 0.88120 to 0.88360, saving model to weights.hdf5\n",
            "Epoch 76/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2988 - acc: 0.8971 - val_loss: 0.4725 - val_acc: 0.8822\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.88360\n",
            "Epoch 77/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2940 - acc: 0.8984 - val_loss: 0.4852 - val_acc: 0.8716\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.88360\n",
            "Epoch 78/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.3000 - acc: 0.8961 - val_loss: 0.4528 - val_acc: 0.8786\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.88360\n",
            "Epoch 79/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.2857 - acc: 0.9000 - val_loss: 0.4469 - val_acc: 0.8803\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.88360\n",
            "Epoch 80/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2931 - acc: 0.8996 - val_loss: 0.4820 - val_acc: 0.8829\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.88360\n",
            "Epoch 81/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2955 - acc: 0.8979 - val_loss: 0.4395 - val_acc: 0.8752\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.88360\n",
            "Epoch 82/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2929 - acc: 0.8989 - val_loss: 0.4378 - val_acc: 0.8794\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.88360\n",
            "Epoch 83/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2967 - acc: 0.8964 - val_loss: 0.4677 - val_acc: 0.8754\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.88360\n",
            "Epoch 84/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2981 - acc: 0.8980 - val_loss: 0.4182 - val_acc: 0.8801\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.88360\n",
            "Epoch 85/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2864 - acc: 0.8997 - val_loss: 0.4664 - val_acc: 0.8759\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.88360\n",
            "Epoch 86/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2888 - acc: 0.8999 - val_loss: 0.4747 - val_acc: 0.8722\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.88360\n",
            "Epoch 87/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2870 - acc: 0.9009 - val_loss: 0.4724 - val_acc: 0.8753\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.88360\n",
            "Epoch 88/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.2876 - acc: 0.9015 - val_loss: 0.4853 - val_acc: 0.8787\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.88360\n",
            "Epoch 89/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2833 - acc: 0.9016 - val_loss: 0.4255 - val_acc: 0.8779\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.88360\n",
            "Epoch 90/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2832 - acc: 0.9020 - val_loss: 0.4958 - val_acc: 0.8792\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.88360\n",
            "Epoch 91/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2833 - acc: 0.9015 - val_loss: 0.4492 - val_acc: 0.8845\n",
            "\n",
            "Epoch 00091: val_acc improved from 0.88360 to 0.88450, saving model to weights.hdf5\n",
            "Epoch 92/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2818 - acc: 0.9029 - val_loss: 0.4241 - val_acc: 0.8822\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.88450\n",
            "Epoch 93/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2777 - acc: 0.9033 - val_loss: 0.5037 - val_acc: 0.8739\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.88450\n",
            "Epoch 94/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2787 - acc: 0.9024 - val_loss: 0.5270 - val_acc: 0.8799\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.88450\n",
            "Epoch 95/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2846 - acc: 0.9012 - val_loss: 0.4662 - val_acc: 0.8813\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.88450\n",
            "Epoch 96/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2770 - acc: 0.9021 - val_loss: 0.4835 - val_acc: 0.8799\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.88450\n",
            "Epoch 97/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2792 - acc: 0.9041 - val_loss: 0.4676 - val_acc: 0.8746\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.88450\n",
            "Epoch 98/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2783 - acc: 0.9042 - val_loss: 0.4731 - val_acc: 0.8791\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.88450\n",
            "Epoch 99/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2840 - acc: 0.9016 - val_loss: 0.4763 - val_acc: 0.8735\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.88450\n",
            "Epoch 100/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2784 - acc: 0.9036 - val_loss: 0.4606 - val_acc: 0.8783\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.88450\n",
            "Epoch 101/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2778 - acc: 0.9036 - val_loss: 0.4669 - val_acc: 0.8799\n",
            "\n",
            "Epoch 00101: val_acc did not improve from 0.88450\n",
            "Epoch 102/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2723 - acc: 0.9059 - val_loss: 0.4687 - val_acc: 0.8828\n",
            "\n",
            "Epoch 00102: val_acc did not improve from 0.88450\n",
            "Epoch 103/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2772 - acc: 0.9027 - val_loss: 0.4713 - val_acc: 0.8766\n",
            "\n",
            "Epoch 00103: val_acc did not improve from 0.88450\n",
            "Epoch 104/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2791 - acc: 0.9039 - val_loss: 0.4726 - val_acc: 0.8799\n",
            "\n",
            "Epoch 00104: val_acc did not improve from 0.88450\n",
            "Epoch 105/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2698 - acc: 0.9056 - val_loss: 0.4746 - val_acc: 0.8824\n",
            "\n",
            "Epoch 00105: val_acc did not improve from 0.88450\n",
            "Epoch 106/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2743 - acc: 0.9051 - val_loss: 0.4522 - val_acc: 0.8811\n",
            "\n",
            "Epoch 00106: val_acc did not improve from 0.88450\n",
            "Epoch 107/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2717 - acc: 0.9063 - val_loss: 0.4408 - val_acc: 0.8899\n",
            "\n",
            "Epoch 00107: val_acc improved from 0.88450 to 0.88990, saving model to weights.hdf5\n",
            "Epoch 108/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2691 - acc: 0.9075 - val_loss: 0.4758 - val_acc: 0.8824\n",
            "\n",
            "Epoch 00108: val_acc did not improve from 0.88990\n",
            "Epoch 109/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.2646 - acc: 0.9077 - val_loss: 0.4792 - val_acc: 0.8798\n",
            "\n",
            "Epoch 00109: val_acc did not improve from 0.88990\n",
            "Epoch 110/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2676 - acc: 0.9082 - val_loss: 0.4525 - val_acc: 0.8834\n",
            "\n",
            "Epoch 00110: val_acc did not improve from 0.88990\n",
            "Epoch 111/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2714 - acc: 0.9061 - val_loss: 0.4838 - val_acc: 0.8812\n",
            "\n",
            "Epoch 00111: val_acc did not improve from 0.88990\n",
            "Epoch 112/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2703 - acc: 0.9079 - val_loss: 0.4844 - val_acc: 0.8777\n",
            "\n",
            "Epoch 00112: val_acc did not improve from 0.88990\n",
            "Epoch 113/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2627 - acc: 0.9091 - val_loss: 0.4722 - val_acc: 0.8847\n",
            "\n",
            "Epoch 00113: val_acc did not improve from 0.88990\n",
            "Epoch 114/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2616 - acc: 0.9100 - val_loss: 0.4534 - val_acc: 0.8861\n",
            "\n",
            "Epoch 00114: val_acc did not improve from 0.88990\n",
            "Epoch 115/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.2612 - acc: 0.9101 - val_loss: 0.5171 - val_acc: 0.8782\n",
            "\n",
            "Epoch 00115: val_acc did not improve from 0.88990\n",
            "Epoch 116/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2633 - acc: 0.9088 - val_loss: 0.5439 - val_acc: 0.8787\n",
            "\n",
            "Epoch 00116: val_acc did not improve from 0.88990\n",
            "Epoch 117/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2630 - acc: 0.9083 - val_loss: 0.5116 - val_acc: 0.8762\n",
            "\n",
            "Epoch 00117: val_acc did not improve from 0.88990\n",
            "Epoch 118/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2635 - acc: 0.9083 - val_loss: 0.4846 - val_acc: 0.8809\n",
            "\n",
            "Epoch 00118: val_acc did not improve from 0.88990\n",
            "Epoch 119/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2589 - acc: 0.9102 - val_loss: 0.4384 - val_acc: 0.8944\n",
            "\n",
            "Epoch 00119: val_acc improved from 0.88990 to 0.89440, saving model to weights.hdf5\n",
            "Epoch 120/350\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.2568 - acc: 0.9119 - val_loss: 0.4826 - val_acc: 0.8835\n",
            "\n",
            "Epoch 00120: val_acc did not improve from 0.89440\n",
            "Epoch 121/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2654 - acc: 0.9098 - val_loss: 0.4613 - val_acc: 0.8892\n",
            "\n",
            "Epoch 00121: val_acc did not improve from 0.89440\n",
            "Epoch 122/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2579 - acc: 0.9097 - val_loss: 0.4445 - val_acc: 0.8868\n",
            "\n",
            "Epoch 00122: val_acc did not improve from 0.89440\n",
            "Epoch 123/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2570 - acc: 0.9118 - val_loss: 0.4857 - val_acc: 0.8851\n",
            "\n",
            "Epoch 00123: val_acc did not improve from 0.89440\n",
            "Epoch 124/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2531 - acc: 0.9132 - val_loss: 0.4667 - val_acc: 0.8859\n",
            "\n",
            "Epoch 00124: val_acc did not improve from 0.89440\n",
            "Epoch 125/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2580 - acc: 0.9108 - val_loss: 0.4411 - val_acc: 0.8863\n",
            "\n",
            "Epoch 00125: val_acc did not improve from 0.89440\n",
            "Epoch 126/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2537 - acc: 0.9124 - val_loss: 0.5334 - val_acc: 0.8751\n",
            "\n",
            "Epoch 00126: val_acc did not improve from 0.89440\n",
            "Epoch 127/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2550 - acc: 0.9114 - val_loss: 0.4710 - val_acc: 0.8912\n",
            "\n",
            "Epoch 00127: val_acc did not improve from 0.89440\n",
            "Epoch 128/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2627 - acc: 0.9081 - val_loss: 0.4785 - val_acc: 0.8800\n",
            "\n",
            "Epoch 00128: val_acc did not improve from 0.89440\n",
            "Epoch 129/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2564 - acc: 0.9103 - val_loss: 0.4538 - val_acc: 0.8921\n",
            "\n",
            "Epoch 00129: val_acc did not improve from 0.89440\n",
            "Epoch 130/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2571 - acc: 0.9119 - val_loss: 0.4604 - val_acc: 0.8903\n",
            "\n",
            "Epoch 00130: val_acc did not improve from 0.89440\n",
            "Epoch 131/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2538 - acc: 0.9124 - val_loss: 0.4726 - val_acc: 0.8884\n",
            "\n",
            "Epoch 00131: val_acc did not improve from 0.89440\n",
            "Epoch 132/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2592 - acc: 0.9106 - val_loss: 0.4832 - val_acc: 0.8934\n",
            "\n",
            "Epoch 00132: val_acc did not improve from 0.89440\n",
            "Epoch 133/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2515 - acc: 0.9130 - val_loss: 0.4982 - val_acc: 0.8664\n",
            "\n",
            "Epoch 00133: val_acc did not improve from 0.89440\n",
            "Epoch 134/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2471 - acc: 0.9150 - val_loss: 0.5008 - val_acc: 0.8847\n",
            "\n",
            "Epoch 00134: val_acc did not improve from 0.89440\n",
            "Epoch 135/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2511 - acc: 0.9129 - val_loss: 0.4364 - val_acc: 0.8881\n",
            "\n",
            "Epoch 00135: val_acc did not improve from 0.89440\n",
            "Epoch 136/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2462 - acc: 0.9149 - val_loss: 0.4996 - val_acc: 0.8853\n",
            "\n",
            "Epoch 00136: val_acc did not improve from 0.89440\n",
            "Epoch 137/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2529 - acc: 0.9132 - val_loss: 0.4536 - val_acc: 0.8904\n",
            "\n",
            "Epoch 00137: val_acc did not improve from 0.89440\n",
            "Epoch 138/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2496 - acc: 0.9139 - val_loss: 0.4996 - val_acc: 0.8796\n",
            "\n",
            "Epoch 00138: val_acc did not improve from 0.89440\n",
            "Epoch 139/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2506 - acc: 0.9140 - val_loss: 0.4998 - val_acc: 0.8796\n",
            "\n",
            "Epoch 00139: val_acc did not improve from 0.89440\n",
            "Epoch 140/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2430 - acc: 0.9154 - val_loss: 0.4755 - val_acc: 0.8888\n",
            "\n",
            "Epoch 00140: val_acc did not improve from 0.89440\n",
            "Epoch 141/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2495 - acc: 0.9127 - val_loss: 0.5211 - val_acc: 0.8790\n",
            "\n",
            "Epoch 00141: val_acc did not improve from 0.89440\n",
            "Epoch 142/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2473 - acc: 0.9141 - val_loss: 0.4706 - val_acc: 0.8886\n",
            "\n",
            "Epoch 00142: val_acc did not improve from 0.89440\n",
            "Epoch 143/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2488 - acc: 0.9150 - val_loss: 0.4943 - val_acc: 0.8836\n",
            "\n",
            "Epoch 00143: val_acc did not improve from 0.89440\n",
            "Epoch 144/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2488 - acc: 0.9145 - val_loss: 0.5144 - val_acc: 0.8825\n",
            "\n",
            "Epoch 00144: val_acc did not improve from 0.89440\n",
            "Epoch 145/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2460 - acc: 0.9148 - val_loss: 0.4095 - val_acc: 0.8966\n",
            "\n",
            "Epoch 00145: val_acc improved from 0.89440 to 0.89660, saving model to weights.hdf5\n",
            "Epoch 146/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2448 - acc: 0.9151 - val_loss: 0.4813 - val_acc: 0.8875\n",
            "\n",
            "Epoch 00146: val_acc did not improve from 0.89660\n",
            "Epoch 147/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2476 - acc: 0.9149 - val_loss: 0.4340 - val_acc: 0.8898\n",
            "\n",
            "Epoch 00147: val_acc did not improve from 0.89660\n",
            "Epoch 148/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2481 - acc: 0.9146 - val_loss: 0.4941 - val_acc: 0.8849\n",
            "\n",
            "Epoch 00148: val_acc did not improve from 0.89660\n",
            "Epoch 149/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2426 - acc: 0.9160 - val_loss: 0.4872 - val_acc: 0.8841\n",
            "\n",
            "Epoch 00149: val_acc did not improve from 0.89660\n",
            "Epoch 150/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2450 - acc: 0.9154 - val_loss: 0.4641 - val_acc: 0.8898\n",
            "\n",
            "Epoch 00150: val_acc did not improve from 0.89660\n",
            "Epoch 151/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2439 - acc: 0.9171 - val_loss: 0.4486 - val_acc: 0.8912\n",
            "\n",
            "Epoch 00151: val_acc did not improve from 0.89660\n",
            "Epoch 152/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2470 - acc: 0.9134 - val_loss: 0.5154 - val_acc: 0.8805\n",
            "\n",
            "Epoch 00152: val_acc did not improve from 0.89660\n",
            "Epoch 153/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2487 - acc: 0.9135 - val_loss: 0.4800 - val_acc: 0.8873\n",
            "\n",
            "Epoch 00153: val_acc did not improve from 0.89660\n",
            "Epoch 154/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2442 - acc: 0.9155 - val_loss: 0.5066 - val_acc: 0.8844\n",
            "\n",
            "Epoch 00154: val_acc did not improve from 0.89660\n",
            "Epoch 155/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2425 - acc: 0.9184 - val_loss: 0.5252 - val_acc: 0.8843\n",
            "\n",
            "Epoch 00155: val_acc did not improve from 0.89660\n",
            "Epoch 156/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2440 - acc: 0.9167 - val_loss: 0.4740 - val_acc: 0.8860\n",
            "\n",
            "Epoch 00156: val_acc did not improve from 0.89660\n",
            "Epoch 157/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2430 - acc: 0.9157 - val_loss: 0.5297 - val_acc: 0.8753\n",
            "\n",
            "Epoch 00157: val_acc did not improve from 0.89660\n",
            "Epoch 158/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2380 - acc: 0.9194 - val_loss: 0.4947 - val_acc: 0.8792\n",
            "\n",
            "Epoch 00158: val_acc did not improve from 0.89660\n",
            "Epoch 159/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2444 - acc: 0.9152 - val_loss: 0.4855 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00159: val_acc did not improve from 0.89660\n",
            "Epoch 160/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2370 - acc: 0.9189 - val_loss: 0.5013 - val_acc: 0.8829\n",
            "\n",
            "Epoch 00160: val_acc did not improve from 0.89660\n",
            "Epoch 161/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2403 - acc: 0.9176 - val_loss: 0.5277 - val_acc: 0.8840\n",
            "\n",
            "Epoch 00161: val_acc did not improve from 0.89660\n",
            "Epoch 162/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2398 - acc: 0.9173 - val_loss: 0.4673 - val_acc: 0.8888\n",
            "\n",
            "Epoch 00162: val_acc did not improve from 0.89660\n",
            "Epoch 163/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2415 - acc: 0.9157 - val_loss: 0.4804 - val_acc: 0.8903\n",
            "\n",
            "Epoch 00163: val_acc did not improve from 0.89660\n",
            "Epoch 164/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2397 - acc: 0.9172 - val_loss: 0.5489 - val_acc: 0.8713\n",
            "\n",
            "Epoch 00164: val_acc did not improve from 0.89660\n",
            "Epoch 165/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2442 - acc: 0.9160 - val_loss: 0.4947 - val_acc: 0.8821\n",
            "\n",
            "Epoch 00165: val_acc did not improve from 0.89660\n",
            "Epoch 166/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2420 - acc: 0.9166 - val_loss: 0.4697 - val_acc: 0.8944\n",
            "\n",
            "Epoch 00166: val_acc did not improve from 0.89660\n",
            "Epoch 167/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2368 - acc: 0.9179 - val_loss: 0.5010 - val_acc: 0.8855\n",
            "\n",
            "Epoch 00167: val_acc did not improve from 0.89660\n",
            "Epoch 168/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2367 - acc: 0.9190 - val_loss: 0.4814 - val_acc: 0.8901\n",
            "\n",
            "Epoch 00168: val_acc did not improve from 0.89660\n",
            "Epoch 169/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2362 - acc: 0.9189 - val_loss: 0.6087 - val_acc: 0.8720\n",
            "\n",
            "Epoch 00169: val_acc did not improve from 0.89660\n",
            "Epoch 170/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2379 - acc: 0.9178 - val_loss: 0.4649 - val_acc: 0.8891\n",
            "\n",
            "Epoch 00170: val_acc did not improve from 0.89660\n",
            "Epoch 171/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2415 - acc: 0.9162 - val_loss: 0.4479 - val_acc: 0.8928\n",
            "\n",
            "Epoch 00171: val_acc did not improve from 0.89660\n",
            "Epoch 172/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2316 - acc: 0.9188 - val_loss: 0.4992 - val_acc: 0.8893\n",
            "\n",
            "Epoch 00172: val_acc did not improve from 0.89660\n",
            "Epoch 173/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2290 - acc: 0.9213 - val_loss: 0.4483 - val_acc: 0.8953\n",
            "\n",
            "Epoch 00173: val_acc did not improve from 0.89660\n",
            "Epoch 174/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2271 - acc: 0.9217 - val_loss: 0.5041 - val_acc: 0.8896\n",
            "\n",
            "Epoch 00174: val_acc did not improve from 0.89660\n",
            "Epoch 175/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2322 - acc: 0.9189 - val_loss: 0.5247 - val_acc: 0.8899\n",
            "\n",
            "Epoch 00175: val_acc did not improve from 0.89660\n",
            "Epoch 176/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2288 - acc: 0.9210 - val_loss: 0.5044 - val_acc: 0.8883\n",
            "\n",
            "Epoch 00176: val_acc did not improve from 0.89660\n",
            "Epoch 177/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2291 - acc: 0.9209 - val_loss: 0.4496 - val_acc: 0.8965\n",
            "\n",
            "Epoch 00177: val_acc did not improve from 0.89660\n",
            "Epoch 178/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2315 - acc: 0.9212 - val_loss: 0.5077 - val_acc: 0.8878\n",
            "\n",
            "Epoch 00178: val_acc did not improve from 0.89660\n",
            "Epoch 179/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2314 - acc: 0.9196 - val_loss: 0.5023 - val_acc: 0.8891\n",
            "\n",
            "Epoch 00179: val_acc did not improve from 0.89660\n",
            "Epoch 180/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2293 - acc: 0.9216 - val_loss: 0.5467 - val_acc: 0.8762\n",
            "\n",
            "Epoch 00180: val_acc did not improve from 0.89660\n",
            "Epoch 181/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2283 - acc: 0.9213 - val_loss: 0.4574 - val_acc: 0.8943\n",
            "\n",
            "Epoch 00181: val_acc did not improve from 0.89660\n",
            "Epoch 182/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2377 - acc: 0.9172 - val_loss: 0.4695 - val_acc: 0.8956\n",
            "\n",
            "Epoch 00182: val_acc did not improve from 0.89660\n",
            "Epoch 183/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2383 - acc: 0.9168 - val_loss: 0.4646 - val_acc: 0.8907\n",
            "\n",
            "Epoch 00183: val_acc did not improve from 0.89660\n",
            "Epoch 184/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2339 - acc: 0.9193 - val_loss: 0.5293 - val_acc: 0.8863\n",
            "\n",
            "Epoch 00184: val_acc did not improve from 0.89660\n",
            "Epoch 185/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2338 - acc: 0.9181 - val_loss: 0.5376 - val_acc: 0.8876\n",
            "\n",
            "Epoch 00185: val_acc did not improve from 0.89660\n",
            "Epoch 186/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2279 - acc: 0.9207 - val_loss: 0.4670 - val_acc: 0.8956\n",
            "\n",
            "Epoch 00186: val_acc did not improve from 0.89660\n",
            "Epoch 187/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2308 - acc: 0.9206 - val_loss: 0.5017 - val_acc: 0.8857\n",
            "\n",
            "Epoch 00187: val_acc did not improve from 0.89660\n",
            "Epoch 188/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2260 - acc: 0.9214 - val_loss: 0.4908 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00188: val_acc did not improve from 0.89660\n",
            "Epoch 189/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2324 - acc: 0.9212 - val_loss: 0.4539 - val_acc: 0.8932\n",
            "\n",
            "Epoch 00189: val_acc did not improve from 0.89660\n",
            "Epoch 190/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2210 - acc: 0.9240 - val_loss: 0.4869 - val_acc: 0.8857\n",
            "\n",
            "Epoch 00190: val_acc did not improve from 0.89660\n",
            "Epoch 191/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2298 - acc: 0.9202 - val_loss: 0.5568 - val_acc: 0.8891\n",
            "\n",
            "Epoch 00191: val_acc did not improve from 0.89660\n",
            "Epoch 192/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2197 - acc: 0.9252 - val_loss: 0.5034 - val_acc: 0.8844\n",
            "\n",
            "Epoch 00192: val_acc did not improve from 0.89660\n",
            "Epoch 193/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2217 - acc: 0.9245 - val_loss: 0.5284 - val_acc: 0.8868\n",
            "\n",
            "Epoch 00193: val_acc did not improve from 0.89660\n",
            "Epoch 194/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2173 - acc: 0.9263 - val_loss: 0.4787 - val_acc: 0.8916\n",
            "\n",
            "Epoch 00194: val_acc did not improve from 0.89660\n",
            "Epoch 195/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2261 - acc: 0.9219 - val_loss: 0.4723 - val_acc: 0.8919\n",
            "\n",
            "Epoch 00195: val_acc did not improve from 0.89660\n",
            "Epoch 196/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.2186 - acc: 0.9233 - val_loss: 0.5063 - val_acc: 0.8934\n",
            "\n",
            "Epoch 00196: val_acc did not improve from 0.89660\n",
            "Epoch 197/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2190 - acc: 0.9240 - val_loss: 0.5404 - val_acc: 0.8872\n",
            "\n",
            "Epoch 00197: val_acc did not improve from 0.89660\n",
            "Epoch 198/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2200 - acc: 0.9240 - val_loss: 0.4745 - val_acc: 0.8972\n",
            "\n",
            "Epoch 00198: val_acc improved from 0.89660 to 0.89720, saving model to weights.hdf5\n",
            "Epoch 199/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2222 - acc: 0.9236 - val_loss: 0.5172 - val_acc: 0.8918\n",
            "\n",
            "Epoch 00199: val_acc did not improve from 0.89720\n",
            "Epoch 200/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2221 - acc: 0.9235 - val_loss: 0.4977 - val_acc: 0.8934\n",
            "\n",
            "Epoch 00200: val_acc did not improve from 0.89720\n",
            "Epoch 201/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2203 - acc: 0.9239 - val_loss: 0.5331 - val_acc: 0.8860\n",
            "\n",
            "Epoch 00201: val_acc did not improve from 0.89720\n",
            "Epoch 202/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2214 - acc: 0.9238 - val_loss: 0.4860 - val_acc: 0.8913\n",
            "\n",
            "Epoch 00202: val_acc did not improve from 0.89720\n",
            "Epoch 203/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2165 - acc: 0.9251 - val_loss: 0.5292 - val_acc: 0.8853\n",
            "\n",
            "Epoch 00203: val_acc did not improve from 0.89720\n",
            "Epoch 204/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2200 - acc: 0.9241 - val_loss: 0.5023 - val_acc: 0.8846\n",
            "\n",
            "Epoch 00204: val_acc did not improve from 0.89720\n",
            "Epoch 205/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2235 - acc: 0.9225 - val_loss: 0.5337 - val_acc: 0.8864\n",
            "\n",
            "Epoch 00205: val_acc did not improve from 0.89720\n",
            "Epoch 206/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2208 - acc: 0.9236 - val_loss: 0.5002 - val_acc: 0.8874\n",
            "\n",
            "Epoch 00206: val_acc did not improve from 0.89720\n",
            "Epoch 207/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2177 - acc: 0.9254 - val_loss: 0.4900 - val_acc: 0.8866\n",
            "\n",
            "Epoch 00207: val_acc did not improve from 0.89720\n",
            "Epoch 208/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2147 - acc: 0.9251 - val_loss: 0.4686 - val_acc: 0.8961\n",
            "\n",
            "Epoch 00208: val_acc did not improve from 0.89720\n",
            "Epoch 209/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2207 - acc: 0.9241 - val_loss: 0.4852 - val_acc: 0.8866\n",
            "\n",
            "Epoch 00209: val_acc did not improve from 0.89720\n",
            "Epoch 210/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2206 - acc: 0.9238 - val_loss: 0.5034 - val_acc: 0.8949\n",
            "\n",
            "Epoch 00210: val_acc did not improve from 0.89720\n",
            "Epoch 211/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2163 - acc: 0.9251 - val_loss: 0.4896 - val_acc: 0.8924\n",
            "\n",
            "Epoch 00211: val_acc did not improve from 0.89720\n",
            "Epoch 212/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2176 - acc: 0.9247 - val_loss: 0.5045 - val_acc: 0.8905\n",
            "\n",
            "Epoch 00212: val_acc did not improve from 0.89720\n",
            "Epoch 213/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2163 - acc: 0.9260 - val_loss: 0.4717 - val_acc: 0.8924\n",
            "\n",
            "Epoch 00213: val_acc did not improve from 0.89720\n",
            "Epoch 214/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2189 - acc: 0.9248 - val_loss: 0.4741 - val_acc: 0.8893\n",
            "\n",
            "Epoch 00214: val_acc did not improve from 0.89720\n",
            "Epoch 215/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2095 - acc: 0.9280 - val_loss: 0.5045 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00215: val_acc did not improve from 0.89720\n",
            "Epoch 216/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2243 - acc: 0.9224 - val_loss: 0.4929 - val_acc: 0.8922\n",
            "\n",
            "Epoch 00216: val_acc did not improve from 0.89720\n",
            "Epoch 217/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2159 - acc: 0.9261 - val_loss: 0.4847 - val_acc: 0.8901\n",
            "\n",
            "Epoch 00217: val_acc did not improve from 0.89720\n",
            "Epoch 218/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2160 - acc: 0.9266 - val_loss: 0.4977 - val_acc: 0.8904\n",
            "\n",
            "Epoch 00218: val_acc did not improve from 0.89720\n",
            "Epoch 219/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2115 - acc: 0.9269 - val_loss: 0.4949 - val_acc: 0.8934\n",
            "\n",
            "Epoch 00219: val_acc did not improve from 0.89720\n",
            "Epoch 220/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2164 - acc: 0.9265 - val_loss: 0.5477 - val_acc: 0.8866\n",
            "\n",
            "Epoch 00220: val_acc did not improve from 0.89720\n",
            "Epoch 221/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2134 - acc: 0.9265 - val_loss: 0.5445 - val_acc: 0.8853\n",
            "\n",
            "Epoch 00221: val_acc did not improve from 0.89720\n",
            "Epoch 222/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2143 - acc: 0.9265 - val_loss: 0.4695 - val_acc: 0.8962\n",
            "\n",
            "Epoch 00222: val_acc did not improve from 0.89720\n",
            "Epoch 223/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2092 - acc: 0.9288 - val_loss: 0.5350 - val_acc: 0.8910\n",
            "\n",
            "Epoch 00223: val_acc did not improve from 0.89720\n",
            "Epoch 224/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2084 - acc: 0.9286 - val_loss: 0.5338 - val_acc: 0.8970\n",
            "\n",
            "Epoch 00224: val_acc did not improve from 0.89720\n",
            "Epoch 225/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2232 - acc: 0.9229 - val_loss: 0.5783 - val_acc: 0.8831\n",
            "\n",
            "Epoch 00225: val_acc did not improve from 0.89720\n",
            "Epoch 226/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2071 - acc: 0.9282 - val_loss: 0.5040 - val_acc: 0.8915\n",
            "\n",
            "Epoch 00226: val_acc did not improve from 0.89720\n",
            "Epoch 227/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2155 - acc: 0.9269 - val_loss: 0.4976 - val_acc: 0.8925\n",
            "\n",
            "Epoch 00227: val_acc did not improve from 0.89720\n",
            "Epoch 228/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2065 - acc: 0.9285 - val_loss: 0.4694 - val_acc: 0.8925\n",
            "\n",
            "Epoch 00228: val_acc did not improve from 0.89720\n",
            "Epoch 229/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2030 - acc: 0.9290 - val_loss: 0.5274 - val_acc: 0.8933\n",
            "\n",
            "Epoch 00229: val_acc did not improve from 0.89720\n",
            "Epoch 230/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2091 - acc: 0.9285 - val_loss: 0.5138 - val_acc: 0.8988\n",
            "\n",
            "Epoch 00230: val_acc improved from 0.89720 to 0.89880, saving model to weights.hdf5\n",
            "Epoch 231/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2167 - acc: 0.9259 - val_loss: 0.4657 - val_acc: 0.8924\n",
            "\n",
            "Epoch 00231: val_acc did not improve from 0.89880\n",
            "Epoch 232/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2068 - acc: 0.9309 - val_loss: 0.4976 - val_acc: 0.8914\n",
            "\n",
            "Epoch 00232: val_acc did not improve from 0.89880\n",
            "Epoch 233/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2050 - acc: 0.9299 - val_loss: 0.4890 - val_acc: 0.8947\n",
            "\n",
            "Epoch 00233: val_acc did not improve from 0.89880\n",
            "Epoch 234/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2083 - acc: 0.9292 - val_loss: 0.5210 - val_acc: 0.8910\n",
            "\n",
            "Epoch 00234: val_acc did not improve from 0.89880\n",
            "Epoch 235/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2076 - acc: 0.9284 - val_loss: 0.5500 - val_acc: 0.8903\n",
            "\n",
            "Epoch 00235: val_acc did not improve from 0.89880\n",
            "Epoch 236/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2102 - acc: 0.9280 - val_loss: 0.4850 - val_acc: 0.8956\n",
            "\n",
            "Epoch 00236: val_acc did not improve from 0.89880\n",
            "Epoch 237/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2074 - acc: 0.9288 - val_loss: 0.5281 - val_acc: 0.8923\n",
            "\n",
            "Epoch 00237: val_acc did not improve from 0.89880\n",
            "Epoch 238/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2115 - acc: 0.9270 - val_loss: 0.5166 - val_acc: 0.8945\n",
            "\n",
            "Epoch 00238: val_acc did not improve from 0.89880\n",
            "Epoch 239/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2065 - acc: 0.9284 - val_loss: 0.5463 - val_acc: 0.8901\n",
            "\n",
            "Epoch 00239: val_acc did not improve from 0.89880\n",
            "Epoch 240/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2056 - acc: 0.9294 - val_loss: 0.5285 - val_acc: 0.8930\n",
            "\n",
            "Epoch 00240: val_acc did not improve from 0.89880\n",
            "Epoch 241/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2098 - acc: 0.9276 - val_loss: 0.5315 - val_acc: 0.8875\n",
            "\n",
            "Epoch 00241: val_acc did not improve from 0.89880\n",
            "Epoch 242/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2016 - acc: 0.9308 - val_loss: 0.4784 - val_acc: 0.8937\n",
            "\n",
            "Epoch 00242: val_acc did not improve from 0.89880\n",
            "Epoch 243/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2064 - acc: 0.9300 - val_loss: 0.4945 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00243: val_acc did not improve from 0.89880\n",
            "Epoch 244/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2109 - acc: 0.9281 - val_loss: 0.4874 - val_acc: 0.8937\n",
            "\n",
            "Epoch 00244: val_acc did not improve from 0.89880\n",
            "Epoch 245/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2025 - acc: 0.9297 - val_loss: 0.4853 - val_acc: 0.8981\n",
            "\n",
            "Epoch 00245: val_acc did not improve from 0.89880\n",
            "Epoch 246/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1975 - acc: 0.9320 - val_loss: 0.4972 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00246: val_acc did not improve from 0.89880\n",
            "Epoch 247/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2029 - acc: 0.9306 - val_loss: 0.4677 - val_acc: 0.8811\n",
            "\n",
            "Epoch 00247: val_acc did not improve from 0.89880\n",
            "Epoch 248/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2042 - acc: 0.9287 - val_loss: 0.5030 - val_acc: 0.8896\n",
            "\n",
            "Epoch 00248: val_acc did not improve from 0.89880\n",
            "Epoch 249/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2124 - acc: 0.9276 - val_loss: 0.5140 - val_acc: 0.8900\n",
            "\n",
            "Epoch 00249: val_acc did not improve from 0.89880\n",
            "Epoch 250/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.2007 - acc: 0.9311 - val_loss: 0.5637 - val_acc: 0.8940\n",
            "\n",
            "Epoch 00250: val_acc did not improve from 0.89880\n",
            "Epoch 251/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2056 - acc: 0.9294 - val_loss: 0.5306 - val_acc: 0.8938\n",
            "\n",
            "Epoch 00251: val_acc did not improve from 0.89880\n",
            "Epoch 252/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2041 - acc: 0.9299 - val_loss: 0.5165 - val_acc: 0.8909\n",
            "\n",
            "Epoch 00252: val_acc did not improve from 0.89880\n",
            "Epoch 253/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2078 - acc: 0.9272 - val_loss: 0.5603 - val_acc: 0.8900\n",
            "\n",
            "Epoch 00253: val_acc did not improve from 0.89880\n",
            "Epoch 254/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.2030 - acc: 0.9300 - val_loss: 0.5641 - val_acc: 0.8835\n",
            "\n",
            "Epoch 00254: val_acc did not improve from 0.89880\n",
            "Epoch 255/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1960 - acc: 0.9321 - val_loss: 0.5321 - val_acc: 0.8943\n",
            "\n",
            "Epoch 00255: val_acc did not improve from 0.89880\n",
            "Epoch 256/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2003 - acc: 0.9314 - val_loss: 0.5152 - val_acc: 0.8967\n",
            "\n",
            "Epoch 00256: val_acc did not improve from 0.89880\n",
            "Epoch 257/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2019 - acc: 0.9312 - val_loss: 0.4711 - val_acc: 0.8983\n",
            "\n",
            "Epoch 00257: val_acc did not improve from 0.89880\n",
            "Epoch 258/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.2020 - acc: 0.9314 - val_loss: 0.5237 - val_acc: 0.8920\n",
            "\n",
            "Epoch 00258: val_acc did not improve from 0.89880\n",
            "Epoch 259/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2007 - acc: 0.9320 - val_loss: 0.4938 - val_acc: 0.8999\n",
            "\n",
            "Epoch 00259: val_acc improved from 0.89880 to 0.89990, saving model to weights.hdf5\n",
            "Epoch 260/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2012 - acc: 0.9304 - val_loss: 0.5102 - val_acc: 0.8954\n",
            "\n",
            "Epoch 00260: val_acc did not improve from 0.89990\n",
            "Epoch 261/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.1994 - acc: 0.9319 - val_loss: 0.5322 - val_acc: 0.8929\n",
            "\n",
            "Epoch 00261: val_acc did not improve from 0.89990\n",
            "Epoch 262/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2062 - acc: 0.9297 - val_loss: 0.4505 - val_acc: 0.8970\n",
            "\n",
            "Epoch 00262: val_acc did not improve from 0.89990\n",
            "Epoch 263/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.2024 - acc: 0.9307 - val_loss: 0.4788 - val_acc: 0.8934\n",
            "\n",
            "Epoch 00263: val_acc did not improve from 0.89990\n",
            "Epoch 264/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.1957 - acc: 0.9328 - val_loss: 0.5089 - val_acc: 0.8933\n",
            "\n",
            "Epoch 00264: val_acc did not improve from 0.89990\n",
            "Epoch 265/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1981 - acc: 0.9330 - val_loss: 0.5505 - val_acc: 0.8895\n",
            "\n",
            "Epoch 00265: val_acc did not improve from 0.89990\n",
            "Epoch 266/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1973 - acc: 0.9314 - val_loss: 0.4928 - val_acc: 0.8919\n",
            "\n",
            "Epoch 00266: val_acc did not improve from 0.89990\n",
            "Epoch 267/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.1916 - acc: 0.9339 - val_loss: 0.5088 - val_acc: 0.8928\n",
            "\n",
            "Epoch 00267: val_acc did not improve from 0.89990\n",
            "Epoch 268/350\n",
            "1562/1562 [==============================] - 27s 18ms/step - loss: 0.1985 - acc: 0.9323 - val_loss: 0.5367 - val_acc: 0.8954\n",
            "\n",
            "Epoch 00268: val_acc did not improve from 0.89990\n",
            "Epoch 269/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.1906 - acc: 0.9348 - val_loss: 0.4712 - val_acc: 0.8966\n",
            "\n",
            "Epoch 00269: val_acc did not improve from 0.89990\n",
            "Epoch 270/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1984 - acc: 0.9312 - val_loss: 0.5531 - val_acc: 0.8903\n",
            "\n",
            "Epoch 00270: val_acc did not improve from 0.89990\n",
            "Epoch 271/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1978 - acc: 0.9315 - val_loss: 0.4834 - val_acc: 0.8936\n",
            "\n",
            "Epoch 00271: val_acc did not improve from 0.89990\n",
            "Epoch 272/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.1925 - acc: 0.9333 - val_loss: 0.5138 - val_acc: 0.8953\n",
            "\n",
            "Epoch 00272: val_acc did not improve from 0.89990\n",
            "Epoch 273/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1901 - acc: 0.9339 - val_loss: 0.5470 - val_acc: 0.8924\n",
            "\n",
            "Epoch 00273: val_acc did not improve from 0.89990\n",
            "Epoch 274/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1990 - acc: 0.9325 - val_loss: 0.5197 - val_acc: 0.8990\n",
            "\n",
            "Epoch 00274: val_acc did not improve from 0.89990\n",
            "Epoch 275/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.1874 - acc: 0.9354 - val_loss: 0.5142 - val_acc: 0.8960\n",
            "\n",
            "Epoch 00275: val_acc did not improve from 0.89990\n",
            "Epoch 276/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1950 - acc: 0.9322 - val_loss: 0.5332 - val_acc: 0.8949\n",
            "\n",
            "Epoch 00276: val_acc did not improve from 0.89990\n",
            "Epoch 277/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1955 - acc: 0.9326 - val_loss: 0.5170 - val_acc: 0.8993\n",
            "\n",
            "Epoch 00277: val_acc did not improve from 0.89990\n",
            "Epoch 278/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1938 - acc: 0.9343 - val_loss: 0.5150 - val_acc: 0.8931\n",
            "\n",
            "Epoch 00278: val_acc did not improve from 0.89990\n",
            "Epoch 279/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1883 - acc: 0.9361 - val_loss: 0.4947 - val_acc: 0.8962\n",
            "\n",
            "Epoch 00279: val_acc did not improve from 0.89990\n",
            "Epoch 280/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1922 - acc: 0.9345 - val_loss: 0.5316 - val_acc: 0.8954\n",
            "\n",
            "Epoch 00280: val_acc did not improve from 0.89990\n",
            "Epoch 281/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1902 - acc: 0.9344 - val_loss: 0.5203 - val_acc: 0.8938\n",
            "\n",
            "Epoch 00281: val_acc did not improve from 0.89990\n",
            "Epoch 282/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1931 - acc: 0.9344 - val_loss: 0.5535 - val_acc: 0.8970\n",
            "\n",
            "Epoch 00282: val_acc did not improve from 0.89990\n",
            "Epoch 283/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.1936 - acc: 0.9344 - val_loss: 0.5265 - val_acc: 0.8978\n",
            "\n",
            "Epoch 00283: val_acc did not improve from 0.89990\n",
            "Epoch 284/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.1935 - acc: 0.9342 - val_loss: 0.5044 - val_acc: 0.8990\n",
            "\n",
            "Epoch 00284: val_acc did not improve from 0.89990\n",
            "Epoch 285/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1927 - acc: 0.9342 - val_loss: 0.5302 - val_acc: 0.8969\n",
            "\n",
            "Epoch 00285: val_acc did not improve from 0.89990\n",
            "Epoch 286/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.2000 - acc: 0.9314 - val_loss: 0.5066 - val_acc: 0.8986\n",
            "\n",
            "Epoch 00286: val_acc did not improve from 0.89990\n",
            "Epoch 287/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1916 - acc: 0.9339 - val_loss: 0.4731 - val_acc: 0.9003\n",
            "\n",
            "Epoch 00287: val_acc improved from 0.89990 to 0.90030, saving model to weights.hdf5\n",
            "Epoch 288/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1836 - acc: 0.9372 - val_loss: 0.5053 - val_acc: 0.8926\n",
            "\n",
            "Epoch 00288: val_acc did not improve from 0.90030\n",
            "Epoch 289/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1898 - acc: 0.9348 - val_loss: 0.4912 - val_acc: 0.9022\n",
            "\n",
            "Epoch 00289: val_acc improved from 0.90030 to 0.90220, saving model to weights.hdf5\n",
            "Epoch 290/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1917 - acc: 0.9336 - val_loss: 0.5112 - val_acc: 0.8938\n",
            "\n",
            "Epoch 00290: val_acc did not improve from 0.90220\n",
            "Epoch 291/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1888 - acc: 0.9359 - val_loss: 0.5885 - val_acc: 0.8908\n",
            "\n",
            "Epoch 00291: val_acc did not improve from 0.90220\n",
            "Epoch 292/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.1873 - acc: 0.9360 - val_loss: 0.5166 - val_acc: 0.8994\n",
            "\n",
            "Epoch 00292: val_acc did not improve from 0.90220\n",
            "Epoch 293/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1913 - acc: 0.9346 - val_loss: 0.5238 - val_acc: 0.8947\n",
            "\n",
            "Epoch 00293: val_acc did not improve from 0.90220\n",
            "Epoch 294/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1832 - acc: 0.9378 - val_loss: 0.5239 - val_acc: 0.9009\n",
            "\n",
            "Epoch 00294: val_acc did not improve from 0.90220\n",
            "Epoch 295/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1879 - acc: 0.9358 - val_loss: 0.5752 - val_acc: 0.8974\n",
            "\n",
            "Epoch 00295: val_acc did not improve from 0.90220\n",
            "Epoch 296/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1864 - acc: 0.9367 - val_loss: 0.4887 - val_acc: 0.8971\n",
            "\n",
            "Epoch 00296: val_acc did not improve from 0.90220\n",
            "Epoch 297/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.1863 - acc: 0.9376 - val_loss: 0.4794 - val_acc: 0.8981\n",
            "\n",
            "Epoch 00297: val_acc did not improve from 0.90220\n",
            "Epoch 298/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1815 - acc: 0.9379 - val_loss: 0.5821 - val_acc: 0.8859\n",
            "\n",
            "Epoch 00298: val_acc did not improve from 0.90220\n",
            "Epoch 299/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1830 - acc: 0.9372 - val_loss: 0.5853 - val_acc: 0.8902\n",
            "\n",
            "Epoch 00299: val_acc did not improve from 0.90220\n",
            "Epoch 300/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.1820 - acc: 0.9384 - val_loss: 0.5018 - val_acc: 0.8995\n",
            "\n",
            "Epoch 00300: val_acc did not improve from 0.90220\n",
            "Epoch 301/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1859 - acc: 0.9369 - val_loss: 0.4738 - val_acc: 0.9015\n",
            "\n",
            "Epoch 00301: val_acc did not improve from 0.90220\n",
            "Epoch 302/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1832 - acc: 0.9372 - val_loss: 0.4690 - val_acc: 0.8999\n",
            "\n",
            "Epoch 00302: val_acc did not improve from 0.90220\n",
            "Epoch 303/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1869 - acc: 0.9356 - val_loss: 0.5189 - val_acc: 0.8945\n",
            "\n",
            "Epoch 00303: val_acc did not improve from 0.90220\n",
            "Epoch 304/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1819 - acc: 0.9373 - val_loss: 0.4773 - val_acc: 0.8969\n",
            "\n",
            "Epoch 00304: val_acc did not improve from 0.90220\n",
            "Epoch 305/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.1849 - acc: 0.9368 - val_loss: 0.5275 - val_acc: 0.8965\n",
            "\n",
            "Epoch 00305: val_acc did not improve from 0.90220\n",
            "Epoch 306/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1815 - acc: 0.9386 - val_loss: 0.5398 - val_acc: 0.8978\n",
            "\n",
            "Epoch 00306: val_acc did not improve from 0.90220\n",
            "Epoch 307/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1838 - acc: 0.9369 - val_loss: 0.5582 - val_acc: 0.8987\n",
            "\n",
            "Epoch 00307: val_acc did not improve from 0.90220\n",
            "Epoch 308/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1852 - acc: 0.9355 - val_loss: 0.4678 - val_acc: 0.8963\n",
            "\n",
            "Epoch 00308: val_acc did not improve from 0.90220\n",
            "Epoch 309/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1804 - acc: 0.9382 - val_loss: 0.5339 - val_acc: 0.8975\n",
            "\n",
            "Epoch 00309: val_acc did not improve from 0.90220\n",
            "Epoch 310/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1839 - acc: 0.9380 - val_loss: 0.5108 - val_acc: 0.8977\n",
            "\n",
            "Epoch 00310: val_acc did not improve from 0.90220\n",
            "Epoch 311/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1825 - acc: 0.9367 - val_loss: 0.4981 - val_acc: 0.8961\n",
            "\n",
            "Epoch 00311: val_acc did not improve from 0.90220\n",
            "Epoch 312/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1806 - acc: 0.9379 - val_loss: 0.5069 - val_acc: 0.8998\n",
            "\n",
            "Epoch 00312: val_acc did not improve from 0.90220\n",
            "Epoch 313/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1798 - acc: 0.9389 - val_loss: 0.4994 - val_acc: 0.9026\n",
            "\n",
            "Epoch 00313: val_acc improved from 0.90220 to 0.90260, saving model to weights.hdf5\n",
            "Epoch 314/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1832 - acc: 0.9373 - val_loss: 0.5342 - val_acc: 0.8986\n",
            "\n",
            "Epoch 00314: val_acc did not improve from 0.90260\n",
            "Epoch 315/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.1774 - acc: 0.9400 - val_loss: 0.5162 - val_acc: 0.8959\n",
            "\n",
            "Epoch 00315: val_acc did not improve from 0.90260\n",
            "Epoch 316/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1835 - acc: 0.9376 - val_loss: 0.5425 - val_acc: 0.8936\n",
            "\n",
            "Epoch 00316: val_acc did not improve from 0.90260\n",
            "Epoch 317/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1813 - acc: 0.9385 - val_loss: 0.4794 - val_acc: 0.9031\n",
            "\n",
            "Epoch 00317: val_acc improved from 0.90260 to 0.90310, saving model to weights.hdf5\n",
            "Epoch 318/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.1809 - acc: 0.9375 - val_loss: 0.4804 - val_acc: 0.8981\n",
            "\n",
            "Epoch 00318: val_acc did not improve from 0.90310\n",
            "Epoch 319/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1805 - acc: 0.9380 - val_loss: 0.6146 - val_acc: 0.8938\n",
            "\n",
            "Epoch 00319: val_acc did not improve from 0.90310\n",
            "Epoch 320/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1820 - acc: 0.9378 - val_loss: 0.5444 - val_acc: 0.9014\n",
            "\n",
            "Epoch 00320: val_acc did not improve from 0.90310\n",
            "Epoch 321/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1820 - acc: 0.9381 - val_loss: 0.5431 - val_acc: 0.8975\n",
            "\n",
            "Epoch 00321: val_acc did not improve from 0.90310\n",
            "Epoch 322/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1850 - acc: 0.9378 - val_loss: 0.5093 - val_acc: 0.8957\n",
            "\n",
            "Epoch 00322: val_acc did not improve from 0.90310\n",
            "Epoch 323/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1695 - acc: 0.9420 - val_loss: 0.6067 - val_acc: 0.8909\n",
            "\n",
            "Epoch 00323: val_acc did not improve from 0.90310\n",
            "Epoch 324/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1825 - acc: 0.9378 - val_loss: 0.5595 - val_acc: 0.8923\n",
            "\n",
            "Epoch 00324: val_acc did not improve from 0.90310\n",
            "Epoch 325/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1769 - acc: 0.9405 - val_loss: 0.4541 - val_acc: 0.9041\n",
            "\n",
            "Epoch 00325: val_acc improved from 0.90310 to 0.90410, saving model to weights.hdf5\n",
            "Epoch 326/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.1837 - acc: 0.9376 - val_loss: 0.4973 - val_acc: 0.9015\n",
            "\n",
            "Epoch 00326: val_acc did not improve from 0.90410\n",
            "Epoch 327/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1792 - acc: 0.9395 - val_loss: 0.5341 - val_acc: 0.8983\n",
            "\n",
            "Epoch 00327: val_acc did not improve from 0.90410\n",
            "Epoch 328/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1758 - acc: 0.9406 - val_loss: 0.5156 - val_acc: 0.8965\n",
            "\n",
            "Epoch 00328: val_acc did not improve from 0.90410\n",
            "Epoch 329/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.1733 - acc: 0.9399 - val_loss: 0.5099 - val_acc: 0.8993\n",
            "\n",
            "Epoch 00329: val_acc did not improve from 0.90410\n",
            "Epoch 330/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1760 - acc: 0.9395 - val_loss: 0.4993 - val_acc: 0.8977\n",
            "\n",
            "Epoch 00330: val_acc did not improve from 0.90410\n",
            "Epoch 331/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1760 - acc: 0.9406 - val_loss: 0.5203 - val_acc: 0.8975\n",
            "\n",
            "Epoch 00331: val_acc did not improve from 0.90410\n",
            "Epoch 332/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1737 - acc: 0.9403 - val_loss: 0.5581 - val_acc: 0.8916\n",
            "\n",
            "Epoch 00332: val_acc did not improve from 0.90410\n",
            "Epoch 333/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1781 - acc: 0.9391 - val_loss: 0.5430 - val_acc: 0.8955\n",
            "\n",
            "Epoch 00333: val_acc did not improve from 0.90410\n",
            "Epoch 334/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.1756 - acc: 0.9401 - val_loss: 0.5781 - val_acc: 0.8848\n",
            "\n",
            "Epoch 00334: val_acc did not improve from 0.90410\n",
            "Epoch 335/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1721 - acc: 0.9412 - val_loss: 0.5244 - val_acc: 0.8971\n",
            "\n",
            "Epoch 00335: val_acc did not improve from 0.90410\n",
            "Epoch 336/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1746 - acc: 0.9412 - val_loss: 0.5019 - val_acc: 0.8994\n",
            "\n",
            "Epoch 00336: val_acc did not improve from 0.90410\n",
            "Epoch 337/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.1749 - acc: 0.9401 - val_loss: 0.5077 - val_acc: 0.8971\n",
            "\n",
            "Epoch 00337: val_acc did not improve from 0.90410\n",
            "Epoch 338/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1718 - acc: 0.9412 - val_loss: 0.5265 - val_acc: 0.8992\n",
            "\n",
            "Epoch 00338: val_acc did not improve from 0.90410\n",
            "Epoch 339/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1706 - acc: 0.9417 - val_loss: 0.5056 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00339: val_acc did not improve from 0.90410\n",
            "Epoch 340/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.1720 - acc: 0.9409 - val_loss: 0.5152 - val_acc: 0.9012\n",
            "\n",
            "Epoch 00340: val_acc did not improve from 0.90410\n",
            "Epoch 341/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1706 - acc: 0.9427 - val_loss: 0.4808 - val_acc: 0.8995\n",
            "\n",
            "Epoch 00341: val_acc did not improve from 0.90410\n",
            "Epoch 342/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1728 - acc: 0.9407 - val_loss: 0.5351 - val_acc: 0.8990\n",
            "\n",
            "Epoch 00342: val_acc did not improve from 0.90410\n",
            "Epoch 343/350\n",
            "1562/1562 [==============================] - 30s 19ms/step - loss: 0.1764 - acc: 0.9397 - val_loss: 0.5175 - val_acc: 0.8990\n",
            "\n",
            "Epoch 00343: val_acc did not improve from 0.90410\n",
            "Epoch 344/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1757 - acc: 0.9406 - val_loss: 0.5024 - val_acc: 0.9025\n",
            "\n",
            "Epoch 00344: val_acc did not improve from 0.90410\n",
            "Epoch 345/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1765 - acc: 0.9402 - val_loss: 0.4715 - val_acc: 0.9017\n",
            "\n",
            "Epoch 00345: val_acc did not improve from 0.90410\n",
            "Epoch 346/350\n",
            "1562/1562 [==============================] - 29s 18ms/step - loss: 0.1720 - acc: 0.9411 - val_loss: 0.5250 - val_acc: 0.8996\n",
            "\n",
            "Epoch 00346: val_acc did not improve from 0.90410\n",
            "Epoch 347/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1717 - acc: 0.9423 - val_loss: 0.5366 - val_acc: 0.8979\n",
            "\n",
            "Epoch 00347: val_acc did not improve from 0.90410\n",
            "Epoch 348/350\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.1689 - acc: 0.9428 - val_loss: 0.4679 - val_acc: 0.9077\n",
            "\n",
            "Epoch 00348: val_acc improved from 0.90410 to 0.90770, saving model to weights.hdf5\n",
            "Epoch 349/350\n",
            "1562/1562 [==============================] - 28s 18ms/step - loss: 0.1656 - acc: 0.9438 - val_loss: 0.4934 - val_acc: 0.9028\n",
            "\n",
            "Epoch 00349: val_acc did not improve from 0.90770\n",
            "Epoch 350/350\n",
            "1562/1562 [==============================] - 29s 19ms/step - loss: 0.1674 - acc: 0.9434 - val_loss: 0.5119 - val_acc: 0.9031\n",
            "\n",
            "Epoch 00350: val_acc did not improve from 0.90770\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHk9pF0K108g",
        "colab_type": "text"
      },
      "source": [
        "### Result Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vQijZeu17Zd",
        "colab_type": "code",
        "outputId": "e9d3e06f-14d7-419c-e301-396ea77fefdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history_callback.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(history_dict['acc']) + 1)\n",
        "\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXPPeXQn21c8",
        "colab_type": "code",
        "outputId": "c541f8da-1433-4801-f081-ffbb6a629f61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "acc_values = history_dict['acc']\n",
        "val_acc_values = history_dict['val_acc']\n",
        "\n",
        "epochs = range(1, len(history_dict['acc']) + 1)\n",
        "\n",
        "plt.plot(epochs, acc_values, 'bo', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc_values, 'b', label='Validation Accuracy')\n",
        "plt.title('Training and validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}